---
layout: page
title: About me 
---


<strong>Keywords: Music Information Retrieval, Deep learning</strong>

<strong>Contact: jongpillee.brian (at) gmail.com</strong>

<strong>Education</strong>
<ul>
  <li><font color="red">2017 - now</font>. PhD Student, <a href="http://mac.kaist.ac.kr/">MAC Lab</a>, <a href="https://ct.kaist.ac.kr/main.php?">CT</a>, <a href="http://www.kaist.ac.kr/html/en/">KAIST</a>, Korea (Advisor: <a href="http://mac.kaist.ac.kr/~juhan/?">Juhan Nam</a>)</li>
  <li><font color="black">2015 - 2017</font>. M.S., <a href="http://mac.kaist.ac.kr/">MAC Lab</a>, <a href="https://ct.kaist.ac.kr/main.php?">CT</a>, <a href="http://www.kaist.ac.kr/html/en/">KAIST</a>, Korea (Advisor: <a href="http://mac.kaist.ac.kr/~juhan/?">Juhan Nam</a>)</li>
  <li><font color="black">2007 - 2015</font>. B.S., EE, <a href="http://www.hanyang.ac.kr/web/eng">Hanyang University</a>, Korea</li>
</ul>

<strong>Publications</strong>

<p>	
	<h4>
		<ul>
  			<li><a href="https://jongpillee.github.io/research/music/auto-tagging/2017/06/05/Sample-level-Deep-Convolutional-Neural-Networks-for-Music-Auto-tagging-Using-Raw-Waveforms.html">Sample-level Deep Convolutional Neural Networks for Music Auto-tagging Using Raw Waveforms</a></li>
  		</ul>
	</h4>
	<h5>
		<font size="2">Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. Our experiments show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million Song Dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency along layer, such as mel-frequency spectrogram that is widely used in music classification systems.</font>
	</h5>
	<strong>Jongpil Lee</strong>, Jiyoung Park, Keunhyoung Luke Kim, Juhan Nam<br>
	<i>Sound and Music Computing Confenrence (SMC) (Accepted), 2017</i> <br> [<a href="https://arxiv.org/abs/1703.01789" target="_blank">arXiv</a>] 
</p>

<p>	
	<h4>
		<ul>
  			<li><a href="https://jongpillee.github.io/research/music/auto-tagging/2017/03/05/Multi-Level-and-Multi-Scale-Feature-Aggregation-Using-Pre-trained-Convolutional-Neural-Networks-for-Music-Auto-tagging.html">Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging</a></li>
  		</ul>
	</h4>
	<h5>
		<font size="2">Music auto-tagging is often handled in a similar manner to image classification by regarding the 2D audio spectrogram as image data. However, music auto-tagging is distinguished from image classification in that the tags are highly diverse and have different levels of abstractions. Considering this issue, we propose a convolutional neural networks (CNN)-based architecture that embraces multi-level and multi-scaled features. The architecture is trained in three steps. First, we conduct supervised feature learning to capture local audio features using a set of CNNs with different input sizes. Second, we extract audio features from each layer of the pre-trained convolutional networks separately and aggregate them altogether given a long audio clip. Finally, we put them into fully-connected networks and make final predictions of the tags. Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the Magnatagatune dataset and the million song dataset. We further show that the proposed architecture is useful in transfer learning.</font>
	</h5>
	<strong>Jongpil Lee</strong>, Juhan Nam<br>
	<i>IEEE Signal Processing Letters (Accepted), 2017</i> <br> [<a href="https://arxiv.org/abs/1703.01793" target="_blank">arXiv</a>] 
</p>


<p>	
	<h4>
		<ul>
  			<li><a href="https://jongpillee.github.io/research/edm/djs-socialnetwork/2016/11/05/The-Effect-of-DJs-Social-Network-on-Music-Popularity.html">The Effect of DJs’ Social Network on Music Popularity</a></li>
  		</ul>
	</h4>
	<h5>
		<font size="2">This research focuses on two distinctive determinants of DJ popularity in Electronic Dance Music (EDM) culture. While one's individual artistic tastes influence the construction of playlists for festivals, social relationships with other DJs also have an effect on the promotion of a DJ’s works. To test this idea, an analysis of the effect of DJs’ social networks and the audio features of popular songs was conducted. We collected and analyzed 713 DJs’ playlist data from 2013 to 2015, consisting of audio clips of 3172 songs. The number of cases where a DJ played another DJ's song was 15759. Our results indicate that DJs tend to play songs composed by DJs within their exclusive groups. This network effect was confirmed while controlling for the audio features of the songs. This research contributes to a better understand of this interesting but unique creative culture by implementing both the social networks of the artists’ communities and their artistic representations.</font>
	</h5>
	Hyeongseok Wi, Kyung hoon Hyun, <strong>Jongpil Lee</strong>, Wonjae Lee<br>
	<i>International Computer Music Conference (ICMC), 2016</i> <br> [<a href="assets/images/The effect of DJ's social networks on music popularity.pdf" target="_blank">PDF</a>] 
</p>


<span class="image left">
			<img src="assets/images/profile.png" title="profile" >
		</span>